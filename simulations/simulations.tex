\section{How to setup a simulation?}

When you perform a simulation, you are making a series of
approximations.  The first comes with the choice of equation set.  As
discussed earlier, for pure hydrodynamics, this often means solving
the Euler rather than the full Navier-Stokes equations.  The next
approximation is made when discretizing the equations, using the 
methods we described in the earlier chapters.  

Astrophysical flows involve a range of length and timescales.  Often
it is not possible to capture all these scales in a single simulation,
so some comprimises must be made.  If you are interested in the
dynamics on the full scale of the system, $L$, then you will resolve
down to some cutoff scale $l$, where $L/l \sim O(10^3\mbox{--}10^4)$.
At scales smaller than $l$, you neglect the flow features.  The
smallest scale of importance is the disipation scale $\lambda \ll L$.

If you are not resolving down to the dissipation scales in our
problem, then you are working in the realm of {\em large eddy
  simulations} (LES; see \cite{schmidt:2014}).  Proper LES requires a
subgrid model to treat the unresolve scales and their feedback onto
your grid.  If you instead rely on numerical diffusion of your method
to do the dissipation, you are doing {\em implicit large eddy
  simulation} or ILES---for many flows, this be sufficient to capture the
dynamics (including turbulence) that are important~\cite{aspden:2008,MargolinRider:2002}.

\section{Dimensionality and picking your resolution}

Nature is three-dimensional, so realistic simulaitons generally
require full 3-d modeling.  Nevertheless, 2-d simulations can be
useful to scope out the problem space, and in particular, to get a
feel for the timescales and resolution needed to resolve smallscale
features.  The major place where dimensionality affects results is for
instabilities and turbulence.  The trend in 2-d is for small scale
vortices to merge and build larger and larger structures (see, e.g.,
\cite{twodturbulence}), while in 3-d motions at the largest scale
cascade down to smaller scales, driving turbulent flow down to the
smallest scales.

Turbulence can be very important in astrophysical flows, and properly
capturing it requires modeling a large range of lengthscales
(physically, the objective is to get a large {\em Reynolds number}:
the ratio of the advective force to the dissipative force).
Generally, you'll just start to get a resolved inertial range over a
decade in wavenumber in your power spectrum at around 512 zones on a
side of your domain (using the methods that we discussed here).  With
fewer points than this, you are unlikely to capture turbulence.

If nuclear burning is modeled, the steepness of the reaction rate will
set a lengthscale.  Burning can also wash out instabilities, a process
sometimes called {\em fire polishing} \cite{timmeswoosley,SNrt}.
Here, if the timescale for reactions is faster than the growth rate of
an instability, the instability is suppressed.  This can set a small
scale cutoff that is well above the viscous dissipation scale.

Hydrostatic equilibrium also puts demands on the resolution.  Recall that
the momentum equation is:
\begin{equation}
\rho \DDt{\Ub} + \nabla p = \rho \gb
\end{equation}
If hydrostatic equilibrium, $\nabla p = \rho \gb$, is not exactly
satisfied, then the residual will appear as a force that generates an
acceleration and non-zero velocities will build up.  A good rule of
thumb is that you need $10$ points per pressure scale height to get a
reasonable hydrostatic balance (this is discussed at bit in
\cite{hse}).

Spatial resolution is often in contention with improved physics---both
can add to the computational cost of a simulation, so for fixed cost,
you have often have to choose between more physics (e.g., larger
reaction network) or more resolution.

An important part of the simulation process is to ensure that your
results are converged.  In a {\em convergence study}, you run the same
simulation at various resolutions and look to see that the major
results (e.g., integral quantities) do not change significantly with
resolution.  Ideally you would show that any results you are
interested are moving toward an asymptotic value.  Even if you are
not converged however, you could use the trends observed with resolution
to help understand the possible error in your simulation.




\section{Boundary conditions}

Another approximation you make is how to treat the boundaries.
Usually one chooses symmetry / reflecting, outflow / zero-gradient,
inflow (specifying the value on the boundary), or periodic.  In the
case of symmetry, the fluid state is reflected and the normal velocity
is reflected with a sign change.  The transverse velocity can be left
as is (a slip wall) or set to zero at the boundary (no-slip wall).  

Generally speaking, you should put your boundaries as far away from
the place where the action is occuring as possible.  For example,
if you are modeling a convective region bounded by stably stratified
flow, then you should have a large buffer of stable atmosphere
bounding it.

Periodic boundary conditions don't magically fix everything either.
With a triply-periodic box (periodic on all boundaries), then you are
confining the flow.  If energy is injected into the box, then there is
no place for it to expand, and this can lead to artifical heating or
compression.

Symmetry / reflecting boundaries provide a simple way to reduce a
problem size, for example, modeling only an octant of a star in 3-d
instead of the full star.  But they also introduce artifacts, since
they force the normal velocity to zero right on the boundary.  This
means that flow through the center of a star (e.g., if we model only
an octant) can be restricted, resulting in artifical flow / convective
features.  Wall heating can also be an issue---forcing the normal
velocity to zero at the symmetry boundary can lead to stagnation of
the flow there, allowing hot spots to develop and not be transported
away.


\MarginPar{characteristic BCs?}




\section{Timestep considerations}

The timestep constraint we discussed (the CFL condition) is required
for stability, but that is different than accuracy.  In particular,
accurately capturing some physical processes (that may appear as
source terms) might require more restrictive timesteps to ensure that they 
are properly coupled with the hydrodynamics.  

An example where this can commonly arise is reactive flows---combining
hydrodynamics with nuclear or chemical reactions.  If the reactions
are very vigorous, then you may run into a situation where you exaust
your fuel in a zone in a single step, without giving the hydrodynamics
a chance to advect new fuel in.  This represents a breakdown of the
coupling of the hydrodynamics and reactions.  A common way to improve
the accuracy here is to monitor the change in mass fractions of a fuel
and set a timestep limit such that:
\begin{equation}
\Delta t_\mathrm{react} = C_\mathrm{react}
\frac{X_\mathrm{fuel}}{|\omegadot_\mathrm{fuel}|}
\end{equation}
where $C_\mathrm{react} < 1$.  The species creation rate,
$\dot{X}_\mathrm{react}$, is typically evaluated using the value from
the previous step.


\section{Convergence and multiphysics}

For a full simulation, with hydrodynamics coupled to other physics,
you should test your code to ensure that it is converging at the rate
that you expect.  Usually in this case there is no analytic solution,
so you need to measure the convergence numerically.  A standard way to
do this is to run pairs of simulations that differ by a factor of two
in resolution, we'll denote the coarser simulation with the subscript
$c$ and the finer simulation with the subscript $f$.  The error is
computed for a field $\phi$ by coarsening the finer simulation by a
factor of two, and computing the norm of the difference:
\begin{equation}
\epsilon \equiv \| \phi_c - \mathcal{C}(\phi_f) \|
\end{equation}
Here, we denote the coarsening operator as $\mathcal{C}(.)$.  In this
case, two simulations give us a single error.  We then repeat this,
taking the fine simulation from this comparison as the coarse for the
next, and using an even finer simulation to generate a new error.  We
can then measure the convergence of a multiphysics simulation by
comparing the errors from the two pairs of the simulations.  This procedure
was used in, e.g., \cite{ABNZ:III,mccorquodalecolella}.

Ideally, a smooth problem should be used, since discontinuities by
definition do not converge with resolution (and limiters, if used,
would kick in there as well).


\section{Computational cost}

The main limitation to your choice of resolution and timestep is
computational cost.  For a three-dimensional simulation in a box with
$N$ zones on a side, the work scales as $O(N^4)$.  The cost to advance
a single timestep is based on the volume, $N^3$, and the number of
timesteps to get you to a fixed simulation time, $t_\mathrm{max}$, is
$t_\mathrm{max}/\Delta t$, but $\Delta t \sim \Delta x / s \sim L/(N
s)$, where $s$ is the fastest information speed.  So the number 
of timesteps increases with $N$, making the total cost $N^4$.
Think about this for a minute---if you double the number of zones, then
the amount of work you need to do increases by 16$\times$.

A common conflict that arises is: Do you do one ``hero'' calculation
or many smaller calculations to understand the sensitivity of your
results to input parameters?  Computing centers that grant allocations
usually setup their queues to favor big jobs that use as much of the
machine as possible (as that's what their funders use as a metric for
their success).


It is important to understand that no single algorithm will offer everything
you need.


\section{I/O}

Another challenge with simulations is data storage.  For a calculation
on a $1024^3$ grid, using double precision, each number you store
takes 8~bytes / zone, so a single variable on the grid will require
8~GB.  For compressible hydro, you will have atleast 5 variables (in
3-d), $\rho$, $(\rho {\bf U})$, and $(\rho E)$, so this is a minimum of 40~GB per
file.  Often you will want some derived quantities (like temperature)
and composition quantities in your output, which will greatly increase
your storage requirements.

For this reason, simulation codes often have different types of
output.  Checkpoint files store all the variables that are needed to
restart the calculation from where you left off---these should be
stored in a (portable) binary format at machine precision.  You don't
need to keep these around forever, perhaps only saving the last few as
necessary to restart your calculation as it works through a computing
center's queue.  Plotfiles store a lot of variables---basically anything
that will be needed for the analysis of the simulation.  Since you don't
need these to restart, you can probably afford to store the variables
at single precision.  Sometimes you might have multiple levels of plotfiles,
storing a few variables (like density) very frequently to allow for 
the production of smooth movies, and storing a lot of variables and 
derived quantities at a much longer cadence.

A perpetual challenge with the analysis and visualization of
simulations is that sometimes you don't know what you are looking for
until the simulation is complete, so you cannot simply do runtime
visualization and not save the raw data as the simulation progresses.
This leads to collections of plotfiles that can easily top 100~TB per
simulation.  Runtime diagnostics can help a little---if you know any
global quantities that you will be interested in over the course of
the simulation (like peak temperature, or total energy) you can
compute these on the fly and simply store a single number per step (or
every $N$ steps) into a file.
